{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:08.698794099Z",
     "start_time": "2023-12-02T14:36:02.823005432Z"
    }
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from typing import *\n",
    "import torch\n",
    "import itertools\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "import torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class ChessDataset(torch.utils.data.Dataset):\n",
    "    X: List[Tuple[str, str]]\n",
    "    def __init__(self, base_path: str, split: str, category: str, transform: Callable[[Tuple[str, str]], Tuple[str, str]] =None):\n",
    "        self.__data = []\n",
    "        \n",
    "        with open(f\"{base_path}/{split}.che-eng.{category}.che\", \"r\") as fin, open(f\"{base_path}/{split}.che-eng.{category}.en\", \"r\") as fout:\n",
    "            for line_in, line_out in zip(fin, fout):\n",
    "                tokens_line_in = line_in.strip()\n",
    "                tokens_line_out = line_out.strip()\n",
    "                if transform is not None:\n",
    "                   tokens_line_in, tokens_line_out = transform((tokens_line_in, tokens_line_out))\n",
    "                self.__data.append((tokens_line_in, tokens_line_out))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.__data)\n",
    "    \n",
    "    def __getitem__(self, idx) -> Tuple[List[str], List[str]]:\n",
    "        return self.__data[idx]\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:08.706075973Z",
     "start_time": "2023-12-02T14:36:08.681794394Z"
    }
   },
   "id": "b06c2aa13b12b520"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Possible idea - add all one-move chess moves"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:08.712972021Z",
     "start_time": "2023-12-02T14:36:08.699711627Z"
    }
   },
   "id": "46403a0602096f3b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Vocab&dataloader logic\n",
    "BASE_PATH = \"../dataset\"\n",
    "SPLITS = [\"train\", \"test\", \"valid\"]\n",
    "CATEGORIES = [\"0attack\", \"0score\", \"0simple\", \"1attack\", \"1score\", \"1simple\", \"2.comparitiveattack\", \"2.comparitivescore\", \"2.comparitivesimple\"]\n",
    "\n",
    "target_token_transform = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "train_datasets = [ChessDataset(BASE_PATH, \"train\", category) for category in CATEGORIES]\n",
    "\n",
    "\n",
    "train_iter = itertools.chain.from_iterable(train_datasets)\n",
    "source_vocab = build_vocab_from_iterator(\n",
    "    map(lambda x: x[0].split(\" \"), train_iter),\n",
    "    min_freq=1,\n",
    "    specials=special_symbols,\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "train_iter = itertools.chain.from_iterable(train_datasets)\n",
    "target_vocab = build_vocab_from_iterator(\n",
    "    map(lambda x: x[1].split(\" \"), train_iter),\n",
    "    min_freq=1,\n",
    "    specials=special_symbols,\n",
    "    special_first=True\n",
    ")\n",
    "\n",
    "source_vocab.set_default_index(UNK_IDX)\n",
    "target_vocab.set_default_index(UNK_IDX)\n",
    "\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "source_transform = sequential_transforms(\n",
    "    lambda x: x.split(\" \"),\n",
    "    source_vocab,\n",
    "    tensor_transform\n",
    ")\n",
    "\n",
    "target_transform = sequential_transforms(\n",
    "    target_token_transform,\n",
    "    target_vocab,\n",
    "    tensor_transform\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_sample = src_sample.rstrip(\"\\n\")\n",
    "        tgt_sample = tgt_sample.rstrip(\"\\n\")\n",
    "        src_batch.append(source_transform(src_sample))\n",
    "        tgt_batch.append(target_transform(tgt_sample))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def get_dataloader_for(split, category, batch_size):\n",
    "    return torch.utils.data.DataLoader(ChessDataset(BASE_PATH, split, category), batch_size=batch_size, collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:14.056547651Z",
     "start_time": "2023-12-02T14:36:08.708963202Z"
    }
   },
   "id": "5f7bd4c272cdd79f"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#model\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: torch.Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "    \n",
    "class TokenEmbedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: torch.Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "    \n",
    "class Seq2SeqTransformer(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = torch.nn.Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = torch.nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: torch.Tensor,\n",
    "                trg: torch.Tensor,\n",
    "                src_mask: torch.Tensor,\n",
    "                tgt_mask: torch.Tensor,\n",
    "                src_padding_mask: torch.Tensor,\n",
    "                tgt_padding_mask: torch.Tensor,\n",
    "                memory_key_padding_mask: torch.Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: torch.Tensor, src_mask: torch.Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:14.120040742Z",
     "start_time": "2023-12-02T14:36:14.059572438Z"
    }
   },
   "id": "9fcab90d6cd646a6"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:14.121533456Z",
     "start_time": "2023-12-02T14:36:14.085628855Z"
    }
   },
   "id": "343981185065f0f5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_epoch(model, optimizer, dataloader, loss_fn):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(dataloader))\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(dataloader))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T14:36:14.122121921Z",
     "start_time": "2023-12-02T14:36:14.099599208Z"
    }
   },
   "id": "225b3feedb71cd41"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def train_model_for_category(category: str, transformer = None):\n",
    "    NUM_EPOCHS = 200\n",
    "    torch.manual_seed(0)\n",
    "    SRC_VOCAB_SIZE = len(source_vocab)\n",
    "    TGT_VOCAB_SIZE = len(target_vocab)\n",
    "    EMB_SIZE = 64\n",
    "    NHEAD = 8\n",
    "    FFN_HID_DIM = 256\n",
    "    BATCH_SIZE = 128\n",
    "    NUM_ENCODER_LAYERS = 3\n",
    "    NUM_DECODER_LAYERS = 3\n",
    "    \n",
    "    if transformer is None:\n",
    "        transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                         NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "        \n",
    "        for p in transformer.parameters():\n",
    "            if p.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    val_dataloader = get_dataloader_for( \"valid\", category, BATCH_SIZE)\n",
    "    dataloader = get_dataloader_for( \"train\", category, BATCH_SIZE)\n",
    "    \n",
    "    transformer = transformer.to(DEVICE)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "    \n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        start_time = timer()\n",
    "        train_loss = train_epoch(transformer, optimizer, dataloader, loss_fn)\n",
    "        end_time = timer()\n",
    "        val_loss = evaluate(transformer, val_dataloader, loss_fn)\n",
    "        print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    return transformer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T17:03:22.934338677Z",
     "start_time": "2023-12-02T17:03:22.881208450Z"
    }
   },
   "id": "effaf6dc250a246d"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/georgerapeanu/Desktop/BBU-Computer-Science/Semester5/Research Project/venv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 9.443, Val loss: 9.273, Epoch time = 52.036s\n",
      "Epoch: 2, Train loss: 9.152, Val loss: 8.969, Epoch time = 48.488s\n",
      "Epoch: 3, Train loss: 8.853, Val loss: 8.666, Epoch time = 48.823s\n",
      "Epoch: 4, Train loss: 8.558, Val loss: 8.372, Epoch time = 49.503s\n",
      "Epoch: 5, Train loss: 8.273, Val loss: 8.090, Epoch time = 48.783s\n",
      "Epoch: 6, Train loss: 7.998, Val loss: 7.820, Epoch time = 49.187s\n",
      "Epoch: 7, Train loss: 7.734, Val loss: 7.563, Epoch time = 48.860s\n",
      "Epoch: 8, Train loss: 7.483, Val loss: 7.321, Epoch time = 48.682s\n",
      "Epoch: 9, Train loss: 7.247, Val loss: 7.093, Epoch time = 48.773s\n",
      "Epoch: 10, Train loss: 7.026, Val loss: 6.881, Epoch time = 49.835s\n",
      "Epoch: 11, Train loss: 6.821, Val loss: 6.687, Epoch time = 48.622s\n",
      "Epoch: 12, Train loss: 6.633, Val loss: 6.510, Epoch time = 48.760s\n",
      "Epoch: 13, Train loss: 6.462, Val loss: 6.351, Epoch time = 48.812s\n",
      "Epoch: 14, Train loss: 6.309, Val loss: 6.210, Epoch time = 48.897s\n",
      "Epoch: 15, Train loss: 6.173, Val loss: 6.088, Epoch time = 48.870s\n",
      "Epoch: 16, Train loss: 6.056, Val loss: 5.984, Epoch time = 48.854s\n",
      "Epoch: 17, Train loss: 5.955, Val loss: 5.895, Epoch time = 49.007s\n",
      "Epoch: 18, Train loss: 5.870, Val loss: 5.820, Epoch time = 48.948s\n",
      "Epoch: 19, Train loss: 5.799, Val loss: 5.759, Epoch time = 48.757s\n",
      "Epoch: 20, Train loss: 5.741, Val loss: 5.708, Epoch time = 48.922s\n",
      "Epoch: 21, Train loss: 5.692, Val loss: 5.665, Epoch time = 48.978s\n",
      "Epoch: 22, Train loss: 5.650, Val loss: 5.627, Epoch time = 48.607s\n",
      "Epoch: 23, Train loss: 5.611, Val loss: 5.588, Epoch time = 48.533s\n",
      "Epoch: 24, Train loss: 5.571, Val loss: 5.548, Epoch time = 49.093s\n",
      "Epoch: 25, Train loss: 5.530, Val loss: 5.508, Epoch time = 48.627s\n",
      "Epoch: 26, Train loss: 5.487, Val loss: 5.466, Epoch time = 48.572s\n",
      "Epoch: 27, Train loss: 5.440, Val loss: 5.422, Epoch time = 48.819s\n",
      "Epoch: 28, Train loss: 5.391, Val loss: 5.377, Epoch time = 49.894s\n",
      "Epoch: 29, Train loss: 5.341, Val loss: 5.330, Epoch time = 48.755s\n",
      "Epoch: 30, Train loss: 5.288, Val loss: 5.284, Epoch time = 49.022s\n",
      "Epoch: 31, Train loss: 5.235, Val loss: 5.237, Epoch time = 48.811s\n",
      "Epoch: 32, Train loss: 5.182, Val loss: 5.191, Epoch time = 49.530s\n",
      "Epoch: 33, Train loss: 5.129, Val loss: 5.148, Epoch time = 49.157s\n",
      "Epoch: 34, Train loss: 5.076, Val loss: 5.104, Epoch time = 48.325s\n",
      "Epoch: 35, Train loss: 5.025, Val loss: 5.062, Epoch time = 48.569s\n",
      "Epoch: 36, Train loss: 4.976, Val loss: 5.024, Epoch time = 48.843s\n",
      "Epoch: 37, Train loss: 4.927, Val loss: 4.984, Epoch time = 49.003s\n",
      "Epoch: 38, Train loss: 4.881, Val loss: 4.949, Epoch time = 48.580s\n",
      "Epoch: 39, Train loss: 4.837, Val loss: 4.917, Epoch time = 48.476s\n",
      "Epoch: 40, Train loss: 4.796, Val loss: 4.885, Epoch time = 48.805s\n",
      "Epoch: 41, Train loss: 4.754, Val loss: 4.855, Epoch time = 48.499s\n",
      "Epoch: 42, Train loss: 4.717, Val loss: 4.826, Epoch time = 48.819s\n",
      "Epoch: 43, Train loss: 4.681, Val loss: 4.801, Epoch time = 49.211s\n",
      "Epoch: 44, Train loss: 4.647, Val loss: 4.777, Epoch time = 48.882s\n",
      "Epoch: 45, Train loss: 4.616, Val loss: 4.754, Epoch time = 48.718s\n",
      "Epoch: 46, Train loss: 4.584, Val loss: 4.733, Epoch time = 48.671s\n",
      "Epoch: 47, Train loss: 4.556, Val loss: 4.715, Epoch time = 48.634s\n",
      "Epoch: 48, Train loss: 4.529, Val loss: 4.698, Epoch time = 48.779s\n",
      "Epoch: 49, Train loss: 4.503, Val loss: 4.681, Epoch time = 48.954s\n",
      "Epoch: 50, Train loss: 4.479, Val loss: 4.665, Epoch time = 48.642s\n",
      "Epoch: 51, Train loss: 4.456, Val loss: 4.650, Epoch time = 48.644s\n",
      "Epoch: 52, Train loss: 4.431, Val loss: 4.638, Epoch time = 48.500s\n",
      "Epoch: 53, Train loss: 4.412, Val loss: 4.623, Epoch time = 48.655s\n",
      "Epoch: 54, Train loss: 4.392, Val loss: 4.610, Epoch time = 48.660s\n",
      "Epoch: 55, Train loss: 4.372, Val loss: 4.597, Epoch time = 48.882s\n",
      "Epoch: 56, Train loss: 4.354, Val loss: 4.586, Epoch time = 48.499s\n",
      "Epoch: 57, Train loss: 4.337, Val loss: 4.575, Epoch time = 48.855s\n",
      "Epoch: 58, Train loss: 4.319, Val loss: 4.567, Epoch time = 48.520s\n",
      "Epoch: 59, Train loss: 4.303, Val loss: 4.556, Epoch time = 48.590s\n",
      "Epoch: 60, Train loss: 4.287, Val loss: 4.544, Epoch time = 48.542s\n",
      "Epoch: 61, Train loss: 4.269, Val loss: 4.538, Epoch time = 48.884s\n",
      "Epoch: 62, Train loss: 4.254, Val loss: 4.527, Epoch time = 48.651s\n",
      "Epoch: 63, Train loss: 4.240, Val loss: 4.520, Epoch time = 48.657s\n",
      "Epoch: 64, Train loss: 4.227, Val loss: 4.512, Epoch time = 48.959s\n",
      "Epoch: 65, Train loss: 4.211, Val loss: 4.508, Epoch time = 48.664s\n",
      "Epoch: 66, Train loss: 4.197, Val loss: 4.503, Epoch time = 48.652s\n",
      "Epoch: 67, Train loss: 4.185, Val loss: 4.495, Epoch time = 48.705s\n",
      "Epoch: 68, Train loss: 4.174, Val loss: 4.489, Epoch time = 48.913s\n",
      "Epoch: 69, Train loss: 4.161, Val loss: 4.483, Epoch time = 48.871s\n",
      "Epoch: 70, Train loss: 4.149, Val loss: 4.477, Epoch time = 48.870s\n",
      "Epoch: 71, Train loss: 4.135, Val loss: 4.473, Epoch time = 48.767s\n",
      "Epoch: 72, Train loss: 4.126, Val loss: 4.469, Epoch time = 48.870s\n",
      "Epoch: 73, Train loss: 4.113, Val loss: 4.464, Epoch time = 49.304s\n",
      "Epoch: 74, Train loss: 4.103, Val loss: 4.462, Epoch time = 48.784s\n",
      "Epoch: 75, Train loss: 4.092, Val loss: 4.457, Epoch time = 48.696s\n",
      "Epoch: 76, Train loss: 4.081, Val loss: 4.454, Epoch time = 48.884s\n",
      "Epoch: 77, Train loss: 4.070, Val loss: 4.451, Epoch time = 48.754s\n",
      "Epoch: 78, Train loss: 4.059, Val loss: 4.448, Epoch time = 48.728s\n",
      "Epoch: 79, Train loss: 4.050, Val loss: 4.445, Epoch time = 48.783s\n",
      "Epoch: 80, Train loss: 4.044, Val loss: 4.444, Epoch time = 48.948s\n",
      "Epoch: 81, Train loss: 4.031, Val loss: 4.441, Epoch time = 48.636s\n",
      "Epoch: 82, Train loss: 4.022, Val loss: 4.441, Epoch time = 48.848s\n",
      "Epoch: 83, Train loss: 4.012, Val loss: 4.441, Epoch time = 48.623s\n",
      "Epoch: 84, Train loss: 4.003, Val loss: 4.438, Epoch time = 48.969s\n",
      "Epoch: 85, Train loss: 3.995, Val loss: 4.431, Epoch time = 48.696s\n",
      "Epoch: 86, Train loss: 3.987, Val loss: 4.429, Epoch time = 48.805s\n",
      "Epoch: 87, Train loss: 3.977, Val loss: 4.427, Epoch time = 48.648s\n",
      "Epoch: 88, Train loss: 3.970, Val loss: 4.426, Epoch time = 48.870s\n",
      "Epoch: 89, Train loss: 3.960, Val loss: 4.425, Epoch time = 48.690s\n",
      "Epoch: 90, Train loss: 3.952, Val loss: 4.427, Epoch time = 48.393s\n",
      "Epoch: 91, Train loss: 3.944, Val loss: 4.422, Epoch time = 49.031s\n",
      "Epoch: 92, Train loss: 3.935, Val loss: 4.423, Epoch time = 48.894s\n",
      "Epoch: 93, Train loss: 3.928, Val loss: 4.423, Epoch time = 48.888s\n",
      "Epoch: 94, Train loss: 3.921, Val loss: 4.421, Epoch time = 48.732s\n",
      "Epoch: 95, Train loss: 3.913, Val loss: 4.419, Epoch time = 48.766s\n",
      "Epoch: 96, Train loss: 3.906, Val loss: 4.419, Epoch time = 48.618s\n",
      "Epoch: 97, Train loss: 3.898, Val loss: 4.419, Epoch time = 49.127s\n",
      "Epoch: 98, Train loss: 3.890, Val loss: 4.420, Epoch time = 48.489s\n",
      "Epoch: 99, Train loss: 3.882, Val loss: 4.420, Epoch time = 49.095s\n",
      "Epoch: 100, Train loss: 3.874, Val loss: 4.422, Epoch time = 48.523s\n",
      "Epoch: 101, Train loss: 3.869, Val loss: 4.417, Epoch time = 48.776s\n",
      "Epoch: 102, Train loss: 3.860, Val loss: 4.417, Epoch time = 48.818s\n",
      "Epoch: 103, Train loss: 3.854, Val loss: 4.419, Epoch time = 49.471s\n",
      "Epoch: 104, Train loss: 3.845, Val loss: 4.419, Epoch time = 48.702s\n",
      "Epoch: 105, Train loss: 3.838, Val loss: 4.419, Epoch time = 48.667s\n",
      "Epoch: 106, Train loss: 3.832, Val loss: 4.420, Epoch time = 48.984s\n",
      "Epoch: 107, Train loss: 3.824, Val loss: 4.421, Epoch time = 48.848s\n",
      "Epoch: 108, Train loss: 3.818, Val loss: 4.419, Epoch time = 48.731s\n",
      "Epoch: 109, Train loss: 3.809, Val loss: 4.418, Epoch time = 48.892s\n",
      "Epoch: 110, Train loss: 3.805, Val loss: 4.418, Epoch time = 49.079s\n",
      "Epoch: 111, Train loss: 3.795, Val loss: 4.420, Epoch time = 48.923s\n",
      "Epoch: 112, Train loss: 3.790, Val loss: 4.418, Epoch time = 48.541s\n",
      "Epoch: 113, Train loss: 3.784, Val loss: 4.420, Epoch time = 48.675s\n",
      "Epoch: 114, Train loss: 3.778, Val loss: 4.420, Epoch time = 48.945s\n",
      "Epoch: 115, Train loss: 3.771, Val loss: 4.423, Epoch time = 48.692s\n",
      "Epoch: 116, Train loss: 3.764, Val loss: 4.420, Epoch time = 48.503s\n",
      "Epoch: 117, Train loss: 3.758, Val loss: 4.420, Epoch time = 48.661s\n",
      "Epoch: 118, Train loss: 3.751, Val loss: 4.423, Epoch time = 48.845s\n",
      "Epoch: 119, Train loss: 3.744, Val loss: 4.421, Epoch time = 48.990s\n",
      "Epoch: 120, Train loss: 3.740, Val loss: 4.421, Epoch time = 48.769s\n",
      "Epoch: 121, Train loss: 3.733, Val loss: 4.420, Epoch time = 48.470s\n",
      "Epoch: 122, Train loss: 3.726, Val loss: 4.423, Epoch time = 48.850s\n",
      "Epoch: 123, Train loss: 3.721, Val loss: 4.424, Epoch time = 48.390s\n",
      "Epoch: 124, Train loss: 3.714, Val loss: 4.424, Epoch time = 48.487s\n",
      "Epoch: 125, Train loss: 3.708, Val loss: 4.427, Epoch time = 48.782s\n",
      "Epoch: 126, Train loss: 3.702, Val loss: 4.427, Epoch time = 49.351s\n",
      "Epoch: 127, Train loss: 3.696, Val loss: 4.426, Epoch time = 48.684s\n",
      "Epoch: 128, Train loss: 3.689, Val loss: 4.427, Epoch time = 48.805s\n",
      "Epoch: 129, Train loss: 3.682, Val loss: 4.426, Epoch time = 48.779s\n",
      "Epoch: 130, Train loss: 3.678, Val loss: 4.425, Epoch time = 48.825s\n",
      "Epoch: 131, Train loss: 3.669, Val loss: 4.426, Epoch time = 48.590s\n",
      "Epoch: 132, Train loss: 3.664, Val loss: 4.429, Epoch time = 49.029s\n",
      "Epoch: 133, Train loss: 3.659, Val loss: 4.428, Epoch time = 48.842s\n",
      "Epoch: 134, Train loss: 3.655, Val loss: 4.432, Epoch time = 48.837s\n",
      "Epoch: 135, Train loss: 3.646, Val loss: 4.428, Epoch time = 48.699s\n",
      "Epoch: 136, Train loss: 3.642, Val loss: 4.431, Epoch time = 48.891s\n",
      "Epoch: 137, Train loss: 3.635, Val loss: 4.431, Epoch time = 48.879s\n",
      "Epoch: 138, Train loss: 3.628, Val loss: 4.430, Epoch time = 48.931s\n",
      "Epoch: 139, Train loss: 3.624, Val loss: 4.433, Epoch time = 48.543s\n",
      "Epoch: 140, Train loss: 3.619, Val loss: 4.431, Epoch time = 48.629s\n",
      "Epoch: 141, Train loss: 3.614, Val loss: 4.432, Epoch time = 48.733s\n",
      "Epoch: 142, Train loss: 3.606, Val loss: 4.434, Epoch time = 48.715s\n",
      "Epoch: 143, Train loss: 3.600, Val loss: 4.435, Epoch time = 48.606s\n",
      "Epoch: 144, Train loss: 3.594, Val loss: 4.435, Epoch time = 48.808s\n",
      "Epoch: 145, Train loss: 3.591, Val loss: 4.434, Epoch time = 48.771s\n",
      "Epoch: 146, Train loss: 3.583, Val loss: 4.439, Epoch time = 48.867s\n",
      "Epoch: 147, Train loss: 3.576, Val loss: 4.440, Epoch time = 48.865s\n",
      "Epoch: 148, Train loss: 3.574, Val loss: 4.439, Epoch time = 48.672s\n",
      "Epoch: 149, Train loss: 3.568, Val loss: 4.440, Epoch time = 48.641s\n",
      "Epoch: 150, Train loss: 3.563, Val loss: 4.441, Epoch time = 48.774s\n",
      "Epoch: 151, Train loss: 3.556, Val loss: 4.441, Epoch time = 48.685s\n",
      "Epoch: 152, Train loss: 3.553, Val loss: 4.442, Epoch time = 48.752s\n",
      "Epoch: 153, Train loss: 3.546, Val loss: 4.446, Epoch time = 48.834s\n",
      "Epoch: 154, Train loss: 3.539, Val loss: 4.446, Epoch time = 48.628s\n",
      "Epoch: 155, Train loss: 3.533, Val loss: 4.445, Epoch time = 48.669s\n",
      "Epoch: 156, Train loss: 3.528, Val loss: 4.446, Epoch time = 48.598s\n",
      "Epoch: 157, Train loss: 3.525, Val loss: 4.448, Epoch time = 48.572s\n",
      "Epoch: 158, Train loss: 3.521, Val loss: 4.449, Epoch time = 48.689s\n",
      "Epoch: 159, Train loss: 3.513, Val loss: 4.451, Epoch time = 48.638s\n",
      "Epoch: 160, Train loss: 3.510, Val loss: 4.452, Epoch time = 49.040s\n",
      "Epoch: 161, Train loss: 3.505, Val loss: 4.452, Epoch time = 48.441s\n",
      "Epoch: 162, Train loss: 3.499, Val loss: 4.457, Epoch time = 49.079s\n",
      "Epoch: 163, Train loss: 3.492, Val loss: 4.461, Epoch time = 48.920s\n",
      "Epoch: 164, Train loss: 3.486, Val loss: 4.460, Epoch time = 48.554s\n",
      "Epoch: 165, Train loss: 3.483, Val loss: 4.459, Epoch time = 48.450s\n",
      "Epoch: 166, Train loss: 3.476, Val loss: 4.466, Epoch time = 48.619s\n",
      "Epoch: 167, Train loss: 3.473, Val loss: 4.466, Epoch time = 48.836s\n",
      "Epoch: 168, Train loss: 3.471, Val loss: 4.468, Epoch time = 48.486s\n",
      "Epoch: 169, Train loss: 3.464, Val loss: 4.471, Epoch time = 48.984s\n",
      "Epoch: 170, Train loss: 3.457, Val loss: 4.470, Epoch time = 48.846s\n",
      "Epoch: 171, Train loss: 3.451, Val loss: 4.477, Epoch time = 49.170s\n",
      "Epoch: 172, Train loss: 3.443, Val loss: 4.475, Epoch time = 49.365s\n",
      "Epoch: 173, Train loss: 3.442, Val loss: 4.472, Epoch time = 49.764s\n",
      "Epoch: 174, Train loss: 3.437, Val loss: 4.474, Epoch time = 48.727s\n",
      "Epoch: 175, Train loss: 3.431, Val loss: 4.475, Epoch time = 48.563s\n",
      "Epoch: 176, Train loss: 3.428, Val loss: 4.478, Epoch time = 49.047s\n",
      "Epoch: 177, Train loss: 3.422, Val loss: 4.479, Epoch time = 48.887s\n",
      "Epoch: 178, Train loss: 3.415, Val loss: 4.483, Epoch time = 48.687s\n",
      "Epoch: 179, Train loss: 3.409, Val loss: 4.482, Epoch time = 48.828s\n",
      "Epoch: 180, Train loss: 3.405, Val loss: 4.487, Epoch time = 48.655s\n",
      "Epoch: 181, Train loss: 3.398, Val loss: 4.486, Epoch time = 48.539s\n",
      "Epoch: 182, Train loss: 3.393, Val loss: 4.486, Epoch time = 48.631s\n",
      "Epoch: 183, Train loss: 3.389, Val loss: 4.489, Epoch time = 48.845s\n",
      "Epoch: 184, Train loss: 3.382, Val loss: 4.491, Epoch time = 48.577s\n",
      "Epoch: 185, Train loss: 3.379, Val loss: 4.493, Epoch time = 48.791s\n",
      "Epoch: 186, Train loss: 3.378, Val loss: 4.500, Epoch time = 48.694s\n",
      "Epoch: 187, Train loss: 3.372, Val loss: 4.494, Epoch time = 48.784s\n",
      "Epoch: 188, Train loss: 3.363, Val loss: 4.496, Epoch time = 48.576s\n",
      "Epoch: 189, Train loss: 3.362, Val loss: 4.498, Epoch time = 48.628s\n",
      "Epoch: 190, Train loss: 3.355, Val loss: 4.501, Epoch time = 48.422s\n",
      "Epoch: 191, Train loss: 3.350, Val loss: 4.500, Epoch time = 48.549s\n",
      "Epoch: 192, Train loss: 3.343, Val loss: 4.503, Epoch time = 48.488s\n",
      "Epoch: 193, Train loss: 3.342, Val loss: 4.507, Epoch time = 48.652s\n",
      "Epoch: 194, Train loss: 3.336, Val loss: 4.505, Epoch time = 48.976s\n",
      "Epoch: 195, Train loss: 3.331, Val loss: 4.502, Epoch time = 49.265s\n",
      "Epoch: 196, Train loss: 3.327, Val loss: 4.505, Epoch time = 48.568s\n",
      "Epoch: 197, Train loss: 3.321, Val loss: 4.511, Epoch time = 48.511s\n",
      "Epoch: 198, Train loss: 3.315, Val loss: 4.511, Epoch time = 48.365s\n",
      "Epoch: 199, Train loss: 3.305, Val loss: 4.515, Epoch time = 48.443s\n",
      "Epoch: 200, Train loss: 3.308, Val loss: 4.518, Epoch time = 48.674s\n"
     ]
    }
   ],
   "source": [
    "model = train_model_for_category(CATEGORIES[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T19:55:15.856796987Z",
     "start_time": "2023-12-02T17:03:23.850001259Z"
    }
   },
   "id": "c4b54b2e7305ef7d"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    print(memory, ys)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "    return ys\n",
    "\n",
    "\n",
    "# actual function to translate input sentence into target language\n",
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = source_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    print(src)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 5, start_symbol=BOS_IDX).flatten()\n",
    "    return \" \".join(target_vocab.lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T19:59:47.200988333Z",
     "start_time": "2023-12-02T19:59:47.105456263Z"
    }
   },
   "id": "76efe144087b50c6"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "ds = ChessDataset(BASE_PATH, \"test\", CATEGORIES[-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T17:01:39.517788225Z",
     "start_time": "2023-12-02T17:01:39.482923682Z"
    }
   },
   "id": "3c71a741c3f7bfc"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "dl = get_dataloader_for(\"test\", CATEGORIES[-1], 16)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T17:01:40.063441664Z",
     "start_time": "2023-12-02T17:01:40.040623756Z"
    }
   },
   "id": "3929f58cbb307311"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2,     2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
      "             2,     2,     2,     2,     2,     2],\n",
      "        [   31,    13,    41,   208,    28,    28,    28,   922,    31,   303,\n",
      "           327,  2171,    20,     5,   187,    28],\n",
      "        [   37,    32,    55,    31,   868,   336,    93,    37,    21,    40,\n",
      "           873,    50,  5318,     0,    17,   253],\n",
      "        [   36,  2990,   657,   154,    55,   134,  1208,    36,    12,    25,\n",
      "            10,    23,     6,    40,     5,    17],\n",
      "        [   45,  1295,     5,    36,    50,     5,    37,    45,  1967,   531,\n",
      "           130,    40,  2289,    73,    22,     5],\n",
      "        [  130,    21,    22,    45,   110,    22,    36,    12,    82,    10,\n",
      "            12,    10,    14,     5,    37,    18],\n",
      "        [   12,   130,  6129,    12,    21,   557,    45,   149,    29,   724,\n",
      "          3570,  1109,  1819,   235,    36,     6],\n",
      "        [  113,    12,    12,    17,    10,     5,    12,     4,   707,    24,\n",
      "             6,    24,  1956,     6,    45,    40],\n",
      "        [  626,     3,    20,     5, 14192,   847,   149,     3,    67,   155,\n",
      "            17,   156,    28,    32,    12,  5878],\n",
      "        [    0,     1,     5,     9,   645,    40,     4,     1,     8,    12,\n",
      "             5,     7,    93,    28,   109,    12],\n",
      "        [    6,     1,  1672,    70,     6,    12,     3,     1,  1936,    84,\n",
      "            23, 12136,   475,     0,    29,    43],\n",
      "        [  112,     1,     4,   104,    32,   112,     1,     1,    50,    32,\n",
      "            24,    37,    21,    33,    37,     3],\n",
      "        [  591,     1,     3,    50,    28,    33,     1,     1,   360,   387,\n",
      "             5,    36,     5,   117,    64,     1],\n",
      "        [    6,     1,     1,   477,    93,   812,     1,     1,     4,    28,\n",
      "            22,    45,    12,   121,   723,     1],\n",
      "        [   55,     1,     1,     6,    28,    14,     1,     1,     3,    36,\n",
      "          5590,    12,   536,    12,    17,     1],\n",
      "        [    9,     1,     1,    32,    36,    27,     1,     1,     1,  3018,\n",
      "           276,  1833,     4,    38,  1188,     1],\n",
      "        [  167,     1,     1,     5,   294,    75,     1,     1,     1,    55,\n",
      "          5403,    20,     3,     3,   491,     1],\n",
      "        [  943,     1,     1,    11,    12,   770,     1,     1,     1,    28,\n",
      "             4,  1009,     1,     1,  5522,     1],\n",
      "        [   10,     1,     1,    37,   170,     8,     1,     1,     1,    36,\n",
      "             3,     8,     1,     1,    32,     1],\n",
      "        [   18,     1,     1,    36,   686,     5,     1,     1,     1,    65,\n",
      "             1,  1783,     1,     1,    41,     1],\n",
      "        [    4,     1,     1,   276,   112,   260,     1,     1,     1,     0,\n",
      "             1,  5703,     1,     1,    10,     1],\n",
      "        [    3,     1,     1,     8,    24,     4,     1,     1,     1,     8,\n",
      "             1,     8,     1,     1,   478,     1],\n",
      "        [    1,     1,     1,    23,    14,     3,     1,     1,     1,   586,\n",
      "             1,   121,     1,     1,   110,     1],\n",
      "        [    1,     1,     1,   685,   278,     1,     1,     1,     1,    10,\n",
      "             1,  6095,     1,     1,     7,     1],\n",
      "        [    1,     1,     1,     4,    24,     1,     1,     1,     1,    22,\n",
      "             1,    20,     1,     1,    10,     1],\n",
      "        [    1,     1,     1,     3,     5,     1,     1,     1,     1,     7,\n",
      "             1,     5,     1,     1,   638,     1],\n",
      "        [    1,     1,     1,     1,   827,     1,     1,     1,     1,   201,\n",
      "             1,   377,     1,     1,    25,     1],\n",
      "        [    1,     1,     1,     1,     6,     1,     1,     1,     1,    55,\n",
      "             1,     7,     1,     1,     5,     1],\n",
      "        [    1,     1,     1,     1,   763,     1,     1,     1,     1,  1682,\n",
      "             1,     8,     1,     1,   538,     1],\n",
      "        [    1,     1,     1,     1,    31,     1,     1,     1,     1,    24,\n",
      "             1,   629,     1,     1,    22,     1],\n",
      "        [    1,     1,     1,     1,    26,     1,     1,     1,     1,  4043,\n",
      "             1,     5,     1,     1,    20,     1],\n",
      "        [    1,     1,     1,     1,    27,     1,     1,     1,     1,     6,\n",
      "             1,  6277,     1,     1,   788,     1],\n",
      "        [    1,     1,     1,     1,    75,     1,     1,     1,     1,    28,\n",
      "             1, 14520,     1,     1,     6,     1],\n",
      "        [    1,     1,     1,     1,   643,     1,     1,     1,     1,   164,\n",
      "             1,     7,     1,     1,    67,     1],\n",
      "        [    1,     1,     1,     1,   116,     1,     1,     1,     1,   140,\n",
      "             1,   256,     1,     1,     5,     1],\n",
      "        [    1,     1,     1,     1,   101,     1,     1,     1,     1,    73,\n",
      "             1,     0,     1,     1,    27,     1],\n",
      "        [    1,     1,     1,     1,     4,     1,     1,     1,     1,   629,\n",
      "             1, 13261,     1,     1,    21,     1],\n",
      "        [    1,     1,     1,     1,     3,     1,     1,     1,     1,    33,\n",
      "             1,    41,     1,     1,    58,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,    38,\n",
      "             1,   710,     1,     1,   124,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     3,\n",
      "             1,     5,     1,     1,     3,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,   368,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     4,     1,     1,     1,     1],\n",
      "        [    1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     3,     1,     1,     1,     1]])\n"
     ]
    }
   ],
   "source": [
    "for elem in dl:\n",
    "    print(elem[1])\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T17:01:40.427303633Z",
     "start_time": "2023-12-02T17:01:40.376757104Z"
    }
   },
   "id": "d50198fc3cef00df"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  2],\n",
      "        [  7],\n",
      "        [  7],\n",
      "        [ 11],\n",
      "        [ 72],\n",
      "        [ 75],\n",
      "        [  8],\n",
      "        [  9],\n",
      "        [106],\n",
      "        [ 75],\n",
      "        [ 10],\n",
      "        [  3]])\n",
      "tensor([[[-6.2782e-01,  4.0830e-01, -2.6158e+00,  2.3101e-01, -1.0214e+00,\n",
      "           7.3075e-01, -5.7491e-01, -1.0021e+00, -2.2564e-01,  3.7325e-01,\n",
      "           1.7065e+00,  2.1601e-01, -2.2015e+00, -7.6639e-02,  2.5875e-01,\n",
      "           1.1678e-01, -2.2391e+00,  1.1143e+00,  3.9884e-01,  6.2058e-01,\n",
      "          -4.9339e-01,  2.2082e-01, -1.6200e+00, -5.8521e-01, -4.4756e-01,\n",
      "          -1.0529e+00, -3.1756e-01,  1.4897e+00,  8.8275e-01,  1.2393e+00,\n",
      "           3.3516e-01,  1.0295e+00, -3.8907e-01,  5.1055e-01,  9.7723e-01,\n",
      "          -1.2917e-02, -7.7711e-02, -7.7420e-01, -4.5730e-01, -4.4206e-01,\n",
      "           7.2881e-01,  1.0394e+00, -5.1978e-01, -9.2642e-01,  4.4839e-01,\n",
      "           9.0434e-01,  3.0082e-01, -5.5173e-01,  9.0435e-02,  6.0864e-01,\n",
      "           4.5940e-01,  3.6979e-01,  2.2506e+00, -1.3346e+00, -1.0078e+00,\n",
      "           7.9516e-01,  6.5722e-01,  3.1060e-01,  1.4665e+00, -1.1014e+00,\n",
      "          -1.7213e-01, -4.7128e-01,  8.2697e-01, -7.0320e-01]],\n",
      "\n",
      "        [[-4.8805e-01,  4.7819e-01, -2.5180e+00,  2.4476e-01, -1.1035e+00,\n",
      "           5.6965e-01, -5.0900e-01, -1.0463e+00, -2.7641e-01,  4.1435e-01,\n",
      "           1.6280e+00,  4.4725e-01, -2.1077e+00, -8.8903e-02,  1.4405e-01,\n",
      "           2.7998e-01, -2.2088e+00,  1.1353e+00,  4.2803e-01,  5.2754e-01,\n",
      "          -7.4941e-01,  2.0690e-01, -1.8379e+00, -6.4703e-01, -5.9153e-01,\n",
      "          -1.0115e+00, -5.0059e-01,  1.4379e+00,  8.4763e-01,  1.2509e+00,\n",
      "           3.8894e-01,  1.0839e+00, -3.1080e-01,  7.8628e-01,  1.0223e+00,\n",
      "           7.5886e-02,  3.8232e-02, -8.9908e-01, -2.0803e-01, -5.6889e-01,\n",
      "           7.1930e-01,  8.2538e-01, -6.1307e-01, -8.6825e-01,  4.5488e-01,\n",
      "           1.0491e+00,  8.0993e-02, -4.1423e-01, -2.6436e-02,  6.0187e-01,\n",
      "           4.4424e-01,  3.5065e-01,  2.2093e+00, -1.2284e+00, -8.9763e-01,\n",
      "           7.5636e-01,  7.5931e-01,  4.1102e-01,  1.5935e+00, -1.1948e+00,\n",
      "          -2.2861e-01, -5.3720e-01,  6.4451e-01, -5.7851e-01]],\n",
      "\n",
      "        [[-3.9772e-01,  3.2725e-01, -2.4569e+00,  1.4344e-01, -9.7221e-01,\n",
      "           4.3311e-01, -4.0008e-01, -1.1108e+00, -2.1557e-01,  4.2450e-01,\n",
      "           1.6698e+00,  5.1098e-01, -2.0993e+00, -1.3408e-01,  1.0606e-01,\n",
      "           2.9868e-01, -2.2760e+00,  1.1410e+00,  4.0598e-01,  5.4373e-01,\n",
      "          -7.4704e-01,  2.1664e-01, -1.8662e+00, -6.1808e-01, -5.7635e-01,\n",
      "          -1.0298e+00, -5.6658e-01,  1.4327e+00,  8.3234e-01,  1.2885e+00,\n",
      "           3.6866e-01,  1.0270e+00, -3.2083e-01,  8.4660e-01,  1.0090e+00,\n",
      "           7.0439e-02,  1.2290e-01, -8.8789e-01, -2.6375e-01, -5.8252e-01,\n",
      "           7.5010e-01,  8.1210e-01, -5.8586e-01, -8.8714e-01,  5.2808e-01,\n",
      "           1.0469e+00,  9.4305e-02, -4.4549e-01, -7.4617e-02,  6.5744e-01,\n",
      "           3.8722e-01,  4.1748e-01,  2.2132e+00, -1.2037e+00, -8.8932e-01,\n",
      "           7.8952e-01,  7.3262e-01,  4.3515e-01,  1.6325e+00, -1.1923e+00,\n",
      "          -2.3870e-01, -5.9234e-01,  5.7623e-01, -5.7905e-01]],\n",
      "\n",
      "        [[-3.9759e-01,  1.3719e-01, -2.5820e+00,  1.2634e-01, -1.0628e+00,\n",
      "           6.6637e-01, -2.4880e-01, -1.2150e+00, -2.9508e-01,  1.7239e-01,\n",
      "           1.7794e+00,  3.9239e-01, -2.2461e+00, -1.2743e-01,  3.0853e-01,\n",
      "           2.4653e-01, -2.2549e+00,  1.1151e+00,  5.1336e-01,  6.9848e-01,\n",
      "          -6.2087e-01,  2.7805e-01, -1.5649e+00, -5.0121e-01, -5.7668e-01,\n",
      "          -1.1010e+00, -4.9866e-01,  1.4517e+00,  7.1606e-01,  1.3004e+00,\n",
      "           2.9752e-01,  9.0293e-01, -4.8779e-01,  5.8287e-01,  9.2375e-01,\n",
      "           1.9321e-01,  3.4082e-01, -8.5756e-01, -5.1697e-01, -4.5346e-01,\n",
      "           9.7021e-01,  8.4776e-01, -4.3867e-01, -9.6163e-01,  4.6587e-01,\n",
      "           9.4487e-01,  2.9235e-01, -4.3518e-01,  2.3442e-02,  7.2019e-01,\n",
      "           3.2104e-01,  5.2075e-01,  2.3240e+00, -1.2096e+00, -9.3095e-01,\n",
      "           7.9106e-01,  4.8287e-01,  3.7577e-01,  1.3782e+00, -1.0559e+00,\n",
      "          -2.5539e-01, -5.3073e-01,  5.9359e-01, -6.9762e-01]],\n",
      "\n",
      "        [[-7.4901e-01,  3.0523e-01, -2.6407e+00, -2.8403e-02, -9.1270e-01,\n",
      "           4.6448e-01, -1.1606e-01, -1.2162e+00,  8.2307e-02,  5.2756e-01,\n",
      "           1.6775e+00,  1.6514e-01, -2.4069e+00, -1.9337e-02,  4.5386e-01,\n",
      "           2.8339e-01, -2.2776e+00,  1.1620e+00,  5.6633e-01,  5.7503e-01,\n",
      "          -3.9841e-01,  2.6466e-01, -1.5118e+00, -5.2157e-01, -2.4426e-01,\n",
      "          -1.2641e+00, -7.2717e-01,  1.4449e+00,  6.4828e-01,  1.3200e+00,\n",
      "           3.5099e-01,  8.8480e-01, -3.8830e-01,  7.3798e-01,  8.9960e-01,\n",
      "          -6.7693e-02,  1.9751e-01, -7.2670e-01, -5.7698e-01, -3.2589e-01,\n",
      "           9.7695e-01,  7.0606e-01, -3.6135e-01, -1.1209e+00,  5.9687e-01,\n",
      "           8.8616e-01,  3.9966e-01, -4.6951e-01, -8.6297e-02,  7.2776e-01,\n",
      "           3.0494e-01,  7.0100e-01,  2.1379e+00, -1.1063e+00, -9.3269e-01,\n",
      "           7.3933e-01,  7.5582e-01,  5.3995e-01,  1.2990e+00, -7.7420e-01,\n",
      "          -5.9560e-01, -6.3165e-01,  3.8951e-01, -8.7561e-01]],\n",
      "\n",
      "        [[-1.0153e+00,  4.8782e-01, -2.6718e+00, -1.9073e-01, -9.0889e-01,\n",
      "           4.1027e-01, -2.7932e-01, -1.2172e+00, -1.4684e-02,  3.6158e-01,\n",
      "           1.7574e+00,  1.3632e-01, -2.0883e+00, -1.2923e-01,  5.3155e-01,\n",
      "           2.8012e-01, -2.3492e+00,  1.3304e+00,  8.7877e-01,  6.5553e-01,\n",
      "          -2.2914e-01,  3.7543e-01, -1.3489e+00, -8.0589e-01, -6.1561e-01,\n",
      "          -1.0135e+00, -2.4451e-01,  1.4135e+00,  8.2775e-01,  1.2607e+00,\n",
      "           3.9761e-01,  1.2123e+00, -3.9494e-01,  8.8414e-01,  5.8368e-01,\n",
      "          -1.3217e-03,  1.9904e-01, -9.0797e-01, -2.2620e-01, -5.7087e-01,\n",
      "           7.4957e-01,  4.7687e-01, -4.5232e-01, -1.2682e+00,  4.5790e-01,\n",
      "           6.4326e-01,  1.3725e-01, -3.2773e-01,  5.7587e-02,  5.7912e-01,\n",
      "           6.5423e-01,  6.1748e-01,  2.4010e+00, -1.1932e+00, -1.0450e+00,\n",
      "           7.8856e-01,  2.8750e-01,  2.7479e-01,  1.1694e+00, -7.5355e-01,\n",
      "          -3.9464e-01, -5.1895e-01,  5.8641e-01, -6.2313e-01]],\n",
      "\n",
      "        [[-6.2601e-01,  4.9605e-01, -2.6936e+00,  6.0027e-02, -1.0300e+00,\n",
      "           3.6742e-01, -4.1844e-01, -1.2858e+00, -6.8351e-02,  3.1199e-01,\n",
      "           1.9437e+00,  2.8211e-01, -2.1255e+00, -1.8222e-01,  2.6609e-01,\n",
      "           1.3613e-01, -2.1829e+00,  9.4766e-01,  6.4000e-01,  7.3102e-01,\n",
      "          -4.2463e-01,  2.1441e-01, -1.5793e+00, -6.7835e-01, -3.1615e-01,\n",
      "          -1.0991e+00, -4.4605e-01,  1.3941e+00,  7.5218e-01,  1.0292e+00,\n",
      "           4.2057e-01,  1.0400e+00, -5.3243e-01,  6.3097e-01,  8.6204e-01,\n",
      "           9.4760e-02,  3.2496e-01, -8.3552e-01, -4.7289e-01, -3.1948e-01,\n",
      "           8.9014e-01,  8.1698e-01, -4.9564e-01, -1.0596e+00,  4.5758e-01,\n",
      "           9.5040e-01,  3.2418e-01, -4.6693e-01,  8.3155e-02,  7.3276e-01,\n",
      "           5.3518e-01,  4.7254e-01,  2.2374e+00, -1.2243e+00, -9.8800e-01,\n",
      "           7.6374e-01,  6.0374e-01,  4.5284e-01,  1.3873e+00, -1.0735e+00,\n",
      "          -4.3837e-01, -5.2208e-01,  6.8469e-01, -6.7636e-01]],\n",
      "\n",
      "        [[-4.2839e-01,  5.2996e-01, -2.7385e+00,  1.2648e-01, -1.1472e+00,\n",
      "           5.4906e-01, -4.9957e-01, -1.3969e+00, -1.0676e-01,  3.5235e-01,\n",
      "           1.7876e+00,  1.2071e-01, -2.1089e+00, -2.1068e-01,  3.7428e-01,\n",
      "           4.4144e-02, -2.1881e+00,  9.7260e-01,  5.4840e-01,  5.5609e-01,\n",
      "          -5.2075e-01,  3.5608e-01, -1.6590e+00, -4.8843e-01, -2.5446e-01,\n",
      "          -1.0208e+00, -3.5608e-01,  1.2938e+00,  8.0788e-01,  1.1015e+00,\n",
      "           4.3184e-01,  1.0625e+00, -5.4164e-01,  5.5748e-01,  9.3365e-01,\n",
      "           4.4875e-02,  1.8872e-01, -6.3931e-01, -5.5685e-01, -2.3664e-01,\n",
      "           9.3401e-01,  9.6451e-01, -5.4210e-01, -9.3563e-01,  4.1983e-01,\n",
      "           9.2675e-01,  4.2341e-01, -4.4793e-01, -5.6795e-02,  6.7072e-01,\n",
      "           5.3008e-01,  5.3611e-01,  2.2389e+00, -1.2722e+00, -1.0818e+00,\n",
      "           7.5049e-01,  6.8536e-01,  4.0347e-01,  1.3428e+00, -1.0167e+00,\n",
      "          -4.6678e-01, -5.6367e-01,  6.8773e-01, -6.8568e-01]],\n",
      "\n",
      "        [[-2.0243e-01,  6.2578e-01, -2.2759e+00,  3.2520e-01, -1.4068e+00,\n",
      "           7.1005e-01, -4.9339e-01, -1.5889e+00, -1.3929e-01,  5.0647e-01,\n",
      "           1.1913e+00, -4.1901e-01, -1.9548e+00, -6.4830e-01,  5.3302e-01,\n",
      "          -1.5725e-01, -2.7861e+00,  1.1214e+00,  6.8398e-01,  5.7338e-01,\n",
      "          -2.8470e-01,  5.5871e-01, -1.7348e+00, -6.6815e-01, -7.1089e-01,\n",
      "          -1.1285e+00, -3.4549e-01,  1.1729e+00,  7.7457e-01,  1.3861e+00,\n",
      "           1.2198e-01,  1.3650e+00, -3.0125e-01,  9.4681e-01,  8.7490e-01,\n",
      "          -1.3552e-01,  5.0037e-03, -3.7773e-01,  6.5569e-02, -4.5072e-01,\n",
      "           9.7749e-01,  3.9990e-01, -3.3777e-01, -8.0573e-01,  6.8619e-01,\n",
      "           8.2409e-01,  2.7027e-01, -4.7496e-01,  4.9206e-02,  7.0970e-01,\n",
      "           3.9810e-01,  6.8970e-01,  2.0595e+00, -1.2196e+00, -1.4035e+00,\n",
      "           8.3289e-01,  2.2526e-01,  6.4909e-01,  1.2744e+00, -4.7577e-01,\n",
      "          -2.8179e-01, -1.2399e-01,  5.9410e-01, -7.5626e-01]],\n",
      "\n",
      "        [[-1.9928e-01,  2.4089e-01, -2.0729e+00,  4.8637e-01, -1.1114e+00,\n",
      "           8.0820e-01, -6.7353e-01, -1.7466e+00,  9.9430e-02, -6.2310e-02,\n",
      "           1.2360e+00, -5.5516e-01, -1.9083e+00, -5.4302e-01,  6.3523e-01,\n",
      "           1.0832e-01, -2.3875e+00,  1.3372e+00,  9.4641e-01,  4.6111e-01,\n",
      "          -2.1969e-01,  4.5309e-01, -1.3587e+00, -1.0463e+00, -7.3288e-01,\n",
      "          -8.4421e-01, -2.0670e-01,  1.0458e+00,  8.9128e-01,  1.6726e+00,\n",
      "           4.0314e-01,  1.2035e+00, -5.5865e-01,  1.1877e+00,  4.8825e-01,\n",
      "           2.8786e-02,  5.5250e-02, -6.3196e-01,  3.6548e-01, -1.9338e-01,\n",
      "           9.0429e-01,  1.7827e-01, -3.8723e-01, -1.2943e+00,  7.5703e-01,\n",
      "           7.2764e-01, -2.6848e-01, -5.1964e-01, -1.6699e-01,  6.8967e-01,\n",
      "           6.1204e-01,  6.4164e-01,  2.3268e+00, -1.6172e+00, -1.3833e+00,\n",
      "           1.0480e+00, -1.2355e-01,  6.3835e-01,  1.1949e+00, -2.0619e-01,\n",
      "          -3.9120e-01, -2.5518e-01,  3.7509e-01, -5.1543e-01]],\n",
      "\n",
      "        [[-6.7486e-01,  3.5016e-01, -2.3793e+00,  9.3341e-02, -1.0549e+00,\n",
      "           7.1719e-01, -7.9462e-01, -1.4308e+00, -2.5069e-02,  2.3259e-01,\n",
      "           1.5908e+00,  1.7454e-02, -2.1144e+00, -3.9872e-01,  1.8422e-01,\n",
      "          -1.4383e-02, -2.2067e+00,  1.0033e+00,  6.7686e-01,  6.6695e-01,\n",
      "          -2.5001e-01,  3.4153e-01, -1.5936e+00, -6.8589e-01, -4.0341e-01,\n",
      "          -1.1228e+00, -1.4034e-01,  1.5116e+00,  9.8474e-01,  1.2612e+00,\n",
      "           3.0943e-01,  1.1812e+00, -3.1286e-01,  5.2618e-01,  8.2920e-01,\n",
      "           3.5436e-02,  2.6484e-01, -7.0642e-01, -5.8882e-01, -9.1766e-02,\n",
      "           7.7446e-01,  7.6295e-01, -5.0911e-01, -1.0668e+00,  4.7062e-01,\n",
      "           1.2044e+00,  2.5179e-01, -2.4287e-01, -3.4341e-02,  9.2988e-01,\n",
      "           7.3805e-01,  5.9448e-01,  2.1415e+00, -1.4143e+00, -1.2855e+00,\n",
      "           7.7596e-01,  3.2851e-01,  5.0048e-01,  1.3292e+00, -9.6671e-01,\n",
      "          -3.8184e-01, -5.0029e-01,  6.1838e-01, -7.4704e-01]],\n",
      "\n",
      "        [[-7.6502e-01,  5.3695e-01, -2.3985e+00,  1.0010e-01, -9.0575e-01,\n",
      "           7.3568e-01, -7.3960e-01, -1.4035e+00, -5.6430e-02,  9.0464e-02,\n",
      "           1.6290e+00, -4.7433e-02, -2.1527e+00, -2.9865e-01,  3.4815e-01,\n",
      "          -1.0820e-01, -2.1635e+00,  9.2167e-01,  6.5417e-01,  5.9009e-01,\n",
      "          -2.8634e-01,  3.2657e-01, -1.6023e+00, -5.4826e-01, -3.1446e-01,\n",
      "          -1.1062e+00, -2.0565e-01,  1.5421e+00,  1.0745e+00,  1.1384e+00,\n",
      "           4.8395e-01,  1.1206e+00, -3.7854e-01,  2.1358e-01,  9.2920e-01,\n",
      "           1.5290e-01,  1.2822e-01, -8.4599e-01, -6.4951e-01, -1.5913e-01,\n",
      "           7.1142e-01,  8.3337e-01, -6.2163e-01, -9.3265e-01,  3.6363e-01,\n",
      "           1.0465e+00,  3.5367e-01, -3.5796e-01, -7.4300e-02,  7.9920e-01,\n",
      "           7.6464e-01,  5.8152e-01,  2.1923e+00, -1.4285e+00, -1.2053e+00,\n",
      "           8.8232e-01,  4.2410e-01,  4.4972e-01,  1.3779e+00, -1.0750e+00,\n",
      "          -4.6844e-01, -2.9647e-01,  8.4267e-01, -6.8885e-01]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>) tensor([[2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "' better was to develop the knight . '"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, ds[20][0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T20:00:14.914984817Z",
     "start_time": "2023-12-02T20:00:14.793909443Z"
    }
   },
   "id": "3917db419b68274e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "'black black pawn h7 h5 <EOM> <EOMH> 17... h5 <EOR> white bishop <EOPA> <EOCA>'"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[1][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-28T17:02:00.011799587Z",
     "start_time": "2023-11-28T17:01:59.941022247Z"
    }
   },
   "id": "36c3b3b8769b4793"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7b554d9ef7577a8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
