{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ0u0gf4fCcY"
   },
   "source": [
    "# Computer Vision and Deep Learning - Laboratory 3\n",
    "\n",
    "In this laboratory session, we'll be diving into deep convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BL1ip6vGgjp4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2k_8jo0Pn4RF"
   },
   "outputs": [],
   "source": [
    "\n",
    "!wget https://www.math.hkust.edu.hk/~masyleung/Teaching/CAS/MATLAB/image/images/cameraman.jpg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBatUXbIfOzt"
   },
   "source": [
    "# Warm-up\n",
    "\n",
    "\n",
    "Let's start by implementing the basic blocks of a convolutional neural network: the convolutional and (optional) the pooling operations. This would be the last \"low-level\" implementation that you'll do for this class.\n",
    "\n",
    "## Convolutions\n",
    "\n",
    "The convolutional layer is the main building block of a convolutional neural network. These layers contain a set of learnable filters, which will learn which features are relevant for the classification problem based on the training data.\n",
    "During the forward pass, each filter (which __must__ have the same depth as the input volume) is slided over the spatial dimensions of the input volume and we compute an element-wise multiplication between the filter weights and the region of interest in the input volume that lies beneath the filter.\n",
    "\n",
    "The hyperparameters of a convolutional layer are:\n",
    "- the filter size F (usually this is an odd value);\n",
    "- the padding amount which will be added to the input volume P;\n",
    "- the stride S (or the step used when sliding across the input volume);\n",
    "- the number of filters k; the depth of each filter must match the depth of the input volume;\n",
    "\n",
    "Given an input volume of shape  ($H_i$, $W_i$, $D$), the convolutional layer will produce an output of shape ($H_o$, $W_o$, $k$), where:\n",
    "\n",
    "\\begin{equation}\n",
    "W_o = \\frac{W_i - F + 2P}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "H_o = \\frac{H_i - F + 2P}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "<img src=\"https://lh6.googleusercontent.com/gZxwFH6mQ5tPjz6LzVbOaNeVuR1NC-BnuemIWO41qnn7r1PvP4qzwXRWC1OJgo2_PD08qaqJ2-VCF3q9laeK885IJwK-dHhpLDkvRZrx4vxrbLDTsKD2iZYM5SFRq4A6XTklk7_h\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qg10tJHJKVCT"
   },
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "  \"\"\"\n",
    "  This function applies the zero padding operation on all the images in the array X\n",
    "  :param X input array of images; this array has a of rank 4 (batch_size, height, width, channels)\n",
    "  :param pad the amount of zeros to be added around around the spatial size of the images\n",
    "  \"\"\"\n",
    "  # hint you might find the function numpy.pad useful for this purpose\n",
    "  # keep in mind that you only need to pad the spatial dimensions (height and width)\n",
    "  # TODO your code here\n",
    "  return X\n",
    "\n",
    "# load the image using Pillow\n",
    "img = cv2.imread('cameraman.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "img = np.asarray(img)\n",
    "\n",
    "# TODO your code here\n",
    "# pad and display the cameraman.jpg image\n",
    "# (if you are using matplotlib to display the image, use cmap='gray' in the imshow function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmXz-pt7gFXn"
   },
   "outputs": [],
   "source": [
    "def convolution(X, W, bias, pad, stride):\n",
    "  \"\"\"\n",
    "  This function applied to convolution operation on the input X of shape (num_samples, iH, iW, iC)\n",
    "  using the filters defined by the W (filter weights) and  (bias) parameters.\n",
    "\n",
    "  :param X - input of shape (num_samples, iH, iW, iC)\n",
    "  :param W - weights, numpy array of shape (fs, fs, iC, k), where fs is the filter size,\n",
    "    iC is the depth of the input volum and k is the number of filters applied on the image\n",
    "  :param biases - numpy array of shape (1, 1, 1, k)\n",
    "  :param pad - hyperparameter, the amount of padding to be applied\n",
    "  :param stride - hyperparameter, the stride of the convolution\n",
    "  \"\"\"\n",
    "\n",
    "  # 0. compute the size of the output activation map and initialize it with zeros\n",
    "\n",
    "  num_samples = X.shape[0]\n",
    "  iW = X.shape[2]\n",
    "  iH = X.shape[1]\n",
    "  f = W.shape[0]\n",
    "\n",
    "  # TODO your code here\n",
    "  # compute the output width (oW), height (oH) and number of channels (oC)\n",
    "  oW = 0\n",
    "  oH = 0\n",
    "  oC = 0\n",
    "  # initialize the output activation map with zeros\n",
    "  activation_map = None\n",
    "  # end TODO your code here\n",
    "\n",
    "  # 1. pad the samples in the input\n",
    "  # TODO your code here, pad X using pad amount\n",
    "  X_padded = None\n",
    "  # end TODO your code here\n",
    "\n",
    "  # go through each input sample\n",
    "  for i in range(num_samples):\n",
    "    # TODO: get the current sample from the input (use X_padded)\n",
    "    X_i = None\n",
    "    # end TODO your code here\n",
    "\n",
    "    # loop over the spatial dimensions\n",
    "    for y in range(oH):\n",
    "      # TODO your code here\n",
    "      # compute the current ROI in the image on which the filter will be applied (y dimension)\n",
    "      # tl_y - the y coordinate of the top left corner of the current region\n",
    "      # br_y - the y coordinate of the bottom right corner of the current region\n",
    "      tl_y = 0\n",
    "      br_y = 0\n",
    "      # end TODO your code here\n",
    "\n",
    "      for x in range(oW):\n",
    "        # TODO your code here\n",
    "        # compute the current ROI in the image on which the filter will be applied (x dimension)\n",
    "        # tl_x - the x coordinate of the top left corner of the current region\n",
    "        # br_x - the x coordinate of the bottom right corner of the current region\n",
    "        tl_x = 0\n",
    "        br_x = 0\n",
    "        # end TODO your code here\n",
    "\n",
    "        for c in range(oC):\n",
    "            # select the current ROI on which the filter will be applied\n",
    "            roi = X_padded[i, tl_y: br_y, tl_x: br_x, :]\n",
    "            w = W[:, :, :, c]\n",
    "            b = bias[:, :, :, c]\n",
    "\n",
    "            # TODO your code here\n",
    "            # apply the filter with the weights w and bias b on the current image roi\n",
    "\n",
    "            # A. compute the elemetwise product between roi and the weights of the filters (np.multiply)\n",
    "            a = None\n",
    "            # B. sum across all the elements of a\n",
    "            a = None\n",
    "            # C. add the bias term\n",
    "            a = None\n",
    "\n",
    "            # D. add the result in the appropriate position of the output activation map\n",
    "            activation_map[0, 0, 0, 0] = a\n",
    "            # end TODO your code here\n",
    "        assert(activation_map.shape == (num_samples, oH, oW, oC))\n",
    "  return activation_map\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(10)\n",
    "# 100 samples of shape (13, 21, 4)\n",
    "X = np.random.randn(100, 13, 21, 4)\n",
    "\n",
    "# 8 filters (last dimension) of shape (3, 3)\n",
    "W = np.random.randn(3, 3, 4, 8)\n",
    "b = np.random.randn(1, 1, 1, 8)\n",
    "\n",
    "am = convolution(X, W, b, pad=1, stride=2)\n",
    "print(\"am's mean =\\n\", np.mean(am))\n",
    "print(\"am[1, 2, 3] =\\n\", am[3,2,1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UN7Ig4gqIgl"
   },
   "source": [
    "Expected output:\n",
    "\n",
    "am's mean =\n",
    " -0.42841306\n",
    "\n",
    "am[1, 2, 3] =\n",
    " [ 1.780819  -6.5181394 -4.3581524 -2.9115834  1.8401672 -3.722643\n",
    " -8.327618  -3.227787 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tzYzhRAgF2n"
   },
   "source": [
    "Now let's analyse the effect of applying some well known filters used in image processing.\n",
    "\n",
    "### Low pass filters\n",
    "Low pass filters are used to keep the low frequency information within an, while reducing the high frequency information. These filters are the basis of image smoothing.\n",
    "\n",
    "Two well known low pass filters are the _mean filter_ and the _Gaussian filter_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2zrftQwuyjE"
   },
   "outputs": [],
   "source": [
    "image = cv2.imread('cameraman.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "# X contains a single image sample\n",
    "X = np.expand_dims(image, axis=0)\n",
    "\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbyFIs-3ghKb"
   },
   "outputs": [],
   "source": [
    "############################################################\n",
    "# MEAN FILTER\n",
    "############################################################\n",
    "\n",
    "bias = np.asarray([0])\n",
    "bias = bias.reshape((1, 1, 1, 1))\n",
    "\n",
    "mean_filter_3 = np.ones(shape=(3, 3, 1, 1), dtype=np.float32)\n",
    "mean_filter_3 = mean_filter_3/9.0\n",
    "\n",
    "mean_filter_9 = np.ones(shape=(9, 9, 1, 1), dtype=np.float32)\n",
    "mean_filter_9 = mean_filter_9/81.0\n",
    "\n",
    "mean_3x3 = convolution(X, mean_filter_3, bias, pad=0, stride=1)\n",
    "mean_9x9 = convolution(X, mean_filter_9, bias, pad=0, stride=1)\n",
    "\n",
    "plt.figure(0)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[:, :, 0], cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mean_3x3[0, :, :, 0], cmap='gray')\n",
    "plt.title('mean filter 3x3')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[:, :, 0], cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mean_9x9[0, :, :, 0], cmap='gray')\n",
    "plt.title('mean filter 9x9')\n",
    "\n",
    "\n",
    "############################################################\n",
    "# GAUSSIAN FILTER\n",
    "############################################################\n",
    "\n",
    "gaussian_filter = np.asarray(\n",
    "    [[1, 2, 1],\n",
    "     [2, 4, 2],\n",
    "     [1, 2, 1]],\n",
    "     dtype=np.float32\n",
    ")\n",
    "gaussian_filter = gaussian_filter.reshape(3, 3, 1, 1)\n",
    "gaussian_filter = gaussian_filter/16.0\n",
    "\n",
    "gaussian_smoothed = convolution(X, gaussian_filter, bias, pad=0, stride=1)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image[:, :, 0], cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(gaussian_smoothed[0,:,:,0], cmap='gray')\n",
    "plt.title('Gaussian filtered')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2cfOfIWz3OJ"
   },
   "source": [
    "__Optional__: Now load a color image and apply the mean filtering and Gaussian filtering on this color image.\n",
    "Not much changes at the call of the convolution operation, you just need to \"play\" with the convolutional kernels configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rrM5aNuGz91v"
   },
   "outputs": [],
   "source": [
    "# TODO your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_JHtqns1395"
   },
   "source": [
    "### High pass filters\n",
    "\n",
    "On the other hand, high pass filters are used to highlight the high frequency information in an image (edges, abrupt changes in intensities).\n",
    "\n",
    "One of the most commonly used high pass filters is the Sobel kernel (depicted below). These filters can be seen as discrete differentiation operators, and they compute an approximation of the gradient (on the horizontal or vertical direction) of the image intensity function.\n",
    "\n",
    "<img src=\"https://i.ytimg.com/vi/W7OpxFbrD84/maxresdefault.jpg\" width=300px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7gsvHVi2pwD"
   },
   "outputs": [],
   "source": [
    "sobel_horiz = np.asarray([[-1, 0, 1],\n",
    "                          [-2, 0, 2],\n",
    "                          [-1, 0, 1]])\n",
    "\n",
    "sobel_vert = sobel_horiz.T\n",
    "\n",
    "sobel_horiz = np.reshape(sobel_horiz, (3, 3, 1, 1))\n",
    "sobel_vert = np.reshape(sobel_vert, (3, 3, 1, 1))\n",
    "\n",
    "sobel_x = convolution(X, sobel_horiz, bias, 0, 1)\n",
    "sobel_y = convolution(X, sobel_vert, bias, 0, 1)\n",
    "\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(image[:, :, 0], cmap='gray')\n",
    "plt.title('Original image')\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(np.abs(sobel_x[0,:,:,0])/np.abs(np.max(sobel_x[0,:,:,0]))*255, cmap='gray')\n",
    "plt.title('Sobel X')\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(np.abs(sobel_y[0,:,:,0])/np.abs(np.max(sobel_y[0,:,:,0]))*255, cmap='gray')\n",
    "plt.title('Sobel Y')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJnU5vvafZQD"
   },
   "source": [
    "# Convolutional Neural Networks in pytorch\n",
    "\n",
    "\n",
    "In this laboratory, you'll be using convolutional neural networks (CNNs) to perform image classification in torch.\n",
    "You'll follow two approaches:\n",
    "- First, you'll design, implement, and train a simple network **from scratch**. However, in practice, you won't get to train an entire CNN from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, the norm is to pre-train a CNN on a very large dataset and then use these weights as an initialization or a fixed feature extractor for the task of interest (transfer learning).\n",
    "- Therefore, in the second part you'll use **transfer learning** to fine-tune an already trained model on your dataset. Transfer learning is a machine learning technique where a model pre-trained on one task is adapted to a different, but related, task. This approach leverages the knowledge and features learned during the initial training to improve performance and reduce data requirements for the new task, making it more efficient and effective.\n",
    "\n",
    "\n",
    "The main pipeline when training a neural network model is:\n",
    "1. \"Get one with the data\". Analyze your input images,\n",
    "2. Define the model (start with something simple in the beginning)\n",
    "3. Define the training setup\n",
    "4. Train the model\n",
    "5. Test and *analyze* the results.\n",
    "*Repeat the steps 2-5*\n",
    "\n",
    "\n",
    "# 1. Datasets and data loaders\n",
    "\n",
    "\n",
    "You will be working with the [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/), which is a 37 category pet dataset with roughly 200 images for each class. The images have large variations in scale, pose, and lighting, and they are annotated with the breed of the pet (37 classes), the head ROI, and pixel-level trimap segmentation.\n",
    "\n",
    "\n",
    "*Datasets* and *DataLoader* are the core pytorch data structures for interacting with your data. Ideally, you would want your data handling code to be completely decoupled from the model training and testing code (you'll often need to evaluate your model on different datasets).\n",
    "\n",
    "\n",
    "``torch.utils.data.Dataset`` stores the actual information about the dataset (the samples and their corresponding ground truth labels), while the torch.``utils.data.DataLoader`` wraps an iterable around the dataset, allowing easy access to the data, automatic batching, and multi-process data loading).\n",
    "\n",
    "\n",
    "For now, you'll use the OxfordPets dataset implementation from [torchvision](https://pytorch.org/vision/stable/generated/torchvision.datasets.OxfordIIITPet.html), but next time you'll be learning how you can create your own custom dataset and how to configure data loaders.\n",
    "torchvision is a popular package that comprises popular datasets, model architectures, and common image transformations for computer vision.\n",
    "\n",
    "\n",
    "Transforms are common image transformations available in the ``torchvision.transforms`` module and can be used to preprocess and augment the input data. They can be chained together using *Compose*.\n",
    "You can also use it to augment your data.\n",
    "Image augmentation generates similar but distinct training examples after a series of random changes to the training images, and can help reduce overfitting.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sc8D3QOgIzLj"
   },
   "outputs": [],
   "source": [
    "# TODO you code here\n",
    "# - create an object of type torchvision.datasets.OxfordIIITPet, download it\n",
    "# - torch.utils.data.DataLoader object\n",
    "# - display some samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV9pXMPGIzcT"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 2. The Convolutional Neural Network\n",
    "\n",
    "## Convolutional Neural Networks for scratch\n",
    "\n",
    "Check the tutorial from reference [[2]](#scrollTo=my1Fk-G5KKmz&line=2&uniqifier=1).\n",
    "\n",
    "You'll define your convolutional neural network by extending the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, which is the base class for all the neural network modules.\n",
    "In the constructor, you define the layers (and their properties) that comprise your module. ``torch.nn`` [package](https://pytorch.org/docs/stable/nn.html) provides classes for the basic layers of a CNN.\n",
    "\n",
    "The function that you need to override is the _forward()_ function in which you specify computation performed at every call (i.e. how are layers chained and how does the data flow over the computational graph). In other words, this defines the forward pass through your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azt4BFMlI2zb"
   },
   "outputs": [],
   "source": [
    "# TODO your code here: define a simple CNN model, pass a single example through the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqXebwmRI6p7"
   },
   "source": [
    "## Transfer learning\n",
    "\n",
    "\n",
    "Check the tutorial from reference [[3]](#scrollTo=my1Fk-G5KKmz&line=2&uniqifier=1).\n",
    "\n",
    "\n",
    "The ``torchvision`` module provides the implementation and pre-trained weights for common neural network architectures.\n",
    "For example, to load the resnet18 architecture and its weights (after training on ImageNet, you can use:\n",
    "\n",
    "\n",
    "```[python]\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "\n",
    "# Using pretrained weights:\n",
    "model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "```\n",
    "\n",
    " Remember from the first lab, that when using a pre-trained model you must preprocess to the image as the images used for training the model. Using the correct preprocessing method is critical and failing to do so may lead to decreased accuracy or incorrect outputs. Each architecture uses a different preprocessing technique, so there is no standard way to achieve this.\n",
    "\n",
    "\n",
    "#### Note (transfer learning training)\n",
    " In the tutorial, you will notice that the authors use model.train() and model.eval() in the training loop. These functions \"tell\" the model how to act when it is being run. In the next lectures, you will learn that some layers (such as dropout, batch normalization, and so on) behave differently during train and evaluation, and hence the model will produce unexpected results if run in the wrong mode. So don't forget these steps.\n",
    "\n",
    "\n",
    " To freeze the weights of the model and train only the rest, you can set requires_grad of the parameters you want to freeze to False.\n",
    "```\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "```\n",
    "\n",
    "\n",
    "On the other hand, the ``torch.no_grad()``context manager that we used in the prvious lab  is used to prevent calculating gradients in the following code block. Usually it is used when you evaluate your model and don’t need to call backward() to calculate the gradients and update the corresponding parameters. In this mode, the result of every computation will have ``requires_grad=False``, even when the inputs have ``requires_grad=True``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DW7VW3NNJxD7"
   },
   "outputs": [],
   "source": [
    "# TODO : your code here\n",
    "# get a pretrained torchvision module, change the last layer,  pass a single example through the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SgWnQCfI3Bz"
   },
   "source": [
    "# 3. Training the model\n",
    "\n",
    "\n",
    "For training, we need to define a loss function and an optimizer. We'll cover optimizers next time, in this laboratory we'll just stick to stochastic gradient descent.\n",
    "\n",
    "\n",
    "Let's first define some concepts:\n",
    "- epoch: an epoch defines a pass through the entire training dataset. The number of epochs (passes of the entire training dataset the machine learning algorithm has completed) is a hyperparameter of your model. An epoch consists of one or more batches.\n",
    "- batch:  a batch defines how many samples your model \"sees\" before updating its weights. In other words, the batch size is the number of samples that will be passed through to the network at one time during its training.\n",
    "- sample: a sample is just a single training example.\n",
    "\n",
    "\n",
    "As you saw in the previous laboratory, a typical training loop looks like this:\n",
    "```\n",
    "\n",
    "\n",
    "optimizer - the chosen optimizer. It holds the current state of the model and will update the parameters based on the computed gradients. Notice that in the constructor of the optimizer you need to pass the parameters of your model and the learning rate.\n",
    "criterion - the chosen loss function.\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):  # num_epochs is a hyperparameter that specifies when is the training process\n",
    "\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0): # iterate over the dataset, now we use data loaders\n",
    "        # get a batch of data (inputs and their corresponding labels)\n",
    "        inputs, labels = data\n",
    "\n",
    "\n",
    "        # IMPORTANT! set the gradients of the tensors to 0. by default torch accumulates the gradients on subsequent backward passes\n",
    "        # if you omit this step, the gradient would be a combination of the old gradient, which you have already used to update the parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "\n",
    "        # perform the forward pass through the network\n",
    "        outputs = net(inputs)\n",
    "       \n",
    "        # apply the loss function to determine how your model performed on this batch\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # start the backprop process. it will compute the gradient of the loss with respect to the graph leaves\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # update the model parameters by calling the step function\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjlfH7uEI9nL"
   },
   "outputs": [],
   "source": [
    "# TODO code to train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FjwKf-oJ62y"
   },
   "outputs": [],
   "source": [
    "# TODO code to train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yBZGbkx1kQX"
   },
   "source": [
    "\n",
    "\n",
    "Now let's examine the effect of the learning rate over the training process.\n",
    "\n",
    "- First, create two plots: one in which you plot, for each epoch, the loss values on the training and the test data (two series on the same graph), and another one in which you plot, for each epoch, the accuracy values on the training and the test data.\n",
    "- Experiment with different values for the learning rate.\n",
    "- Then, experiment with a torch.optim.lr_scheduler to adjust the learning rate during the training process [doc](!https://pytorch.org/docs/stable/optim.html).\n",
    "\n",
    "```\n",
    "optimizer = SGD(model, lr)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # apply the learning rate scheduler\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "Plot the learning curves for all the training that you performed.\n",
    "Fill in the table to compare the accuracy of your trained models.\n",
    "\n",
    "| Model              | lr config            | accuracy  train| accuracy test |\n",
    "| -----------        | -----------          | ------         | -----         |\n",
    "| Model              | lr info              |   acc          |acc            |\n",
    "| Model              | lr info              |   acc          |acc            |\n",
    "\n",
    "\n",
    "You can work in teams and each team will train the model with a different setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uycj0PIUMc2_"
   },
   "source": [
    "# Using the GPU\n",
    "\n",
    "``torch`` is designed to allow for computation both on CPU and on GPU.\n",
    "If your system has a GPU and the required libraries configured for torch compatibility, the cell below will print information about its state.\n",
    "\n",
    "If you are running your code on colab, you can enable GPU computation from: Runtime->Change Runtime type -> T4 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WI_K2aWkMaaY"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    !nvidia-smi\n",
    "else:\n",
    "    print(\"NO GPU ☹️\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSSXI-3iNc7e"
   },
   "source": [
    "Now we can start to use accelaration.\n",
    "You now need to explictly specify on which device your tensors reside. You can\n",
    "move all of the model's parameters `.to` a certain device (the GPU)\n",
    "and also move the data on the same device there as well\n",
    "before applying the model and calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GwlrUVINOWLu"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "loss_fn(model(x.to(device)), y.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "my1Fk-G5KKmz"
   },
   "source": [
    "#Useful references\n",
    "\n",
    "- [1] [a \"recipe\" ](http://karpathy.github.io/2019/04/25/recipe/)  when you will start training artifcial neural networks;\n",
    "- [2] [Defining a CNN](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) in torch;\n",
    "- [3] [Transfer learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) in torch;\n",
    "- [4] [model debugging](https://developers.google.com/machine-learning/testing-debugging/common/overview)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heKTIeINgByi"
   },
   "source": [
    "# <font color='red'> Optional </font>  \n",
    "## Pooling\n",
    "\n",
    "The pooling layer is used to reduce the spatial dimension of the activation maps, and thus the computational burden. It has no learnable parameters and it operates individually across each input channel and resizes it spatially.\n",
    "\n",
    "The two most common types of pooling are max pooling and average pooling.\n",
    "\n",
    "\n",
    "The hyperparameters of a pooling layer are:\n",
    "- the filter size F (usually this is an odd value);\n",
    "- the stride S (or the step used when sliding across the input volume);\n",
    "\n",
    "Given an input volume of shape  ($H_i$, $W_i$, $D$), the convolutional layer will produce an output of shape ($H_o$, $W_o$, $D$), where:\n",
    "\n",
    "\\begin{equation}\n",
    "W_o = \\frac{W_i - F}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "H_o = \\frac{H_i - F}{S} + 1\n",
    "\\end{equation}\n",
    "\n",
    "An illustration of the pooling operation is depicted in the image below:\n",
    "\n",
    "![picture](https://www.researchgate.net/profile/Alla-Eddine-Guissous/publication/337336341/figure/fig15/AS:855841334898691@1581059883782/Example-for-the-max-pooling-and-the-average-pooling-with-a-filter-size-of-22-and-a.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t81pBIwF5lzv"
   },
   "outputs": [],
   "source": [
    "def pooling(X, filter_size, stride, type):\n",
    "     \"\"\"\n",
    "    Implements the pooling operation\n",
    "\n",
    "    :param X - input volume of shape (num_samples, H, W, C)\n",
    "    :param filter_size - the size of the pooling\n",
    "    :param stride - the stride of the pooling operation\n",
    "    :param type - can be 'max' or 'avg'; the type of the pooling operation to apply\n",
    "\n",
    "    Returns the output of the pooling operation.\n",
    "    \"\"\"\n",
    "  # TODO your code here implement the pooling operation\n",
    "  # you can inspire yourself from the convolution implementation on how to organize your code\n",
    "  pass\n",
    "\n",
    "# TODO your code here\n",
    "# apply the pooling operation on a grayscale image and on a color image\n",
    "# try different values for the stride and filter size. What do you observe?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
